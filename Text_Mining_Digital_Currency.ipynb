{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Text Mining - Digital Currency",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMV+cFDxHOns9ItJqPR09nw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrybrew/bi-15-maret/blob/main/Text_Mining_Digital_Currency.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-mrTVCfmbpX"
      },
      "source": [
        "## **Text Mining**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6v1llWX1ybs"
      },
      "source": [
        "### **Install and Load Packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orcAloXKEtUi"
      },
      "source": [
        "# Install packages\r\n",
        "! pip install tweet-preprocessor\r\n",
        "! pip install pyLDAvis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFNTlh7j0yW-"
      },
      "source": [
        "# Load packages\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import preprocessor as p\r\n",
        "import numpy as np\r\n",
        "import networkx as nx\r\n",
        "import wordcloud\r\n",
        "import nltk\r\n",
        "nltk.download('omw-1.4')\r\n",
        "import warnings\r\n",
        "import itertools\r\n",
        "import re\r\n",
        "import os\r\n",
        "import random\r\n",
        "import pyLDAvis\r\n",
        "import pyLDAvis.sklearn "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0WPn6T5IYLO"
      },
      "source": [
        "# Import module\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from wordcloud import WordCloud\r\n",
        "from wordcloud import STOPWORDS\r\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer \r\n",
        "from tqdm import tqdm\r\n",
        "from nltk import bigrams\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\r\n",
        "from sklearn.decomposition import LatentDirichletAllocation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-TGl07g1GR_"
      },
      "source": [
        "# Set parameter\r\n",
        "warnings.filterwarnings('ignore')\r\n",
        "pyLDAvis.enable_notebook()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POd_N1Vx18Ft"
      },
      "source": [
        "### **Import Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uX4JNHdFmLbd"
      },
      "source": [
        "# Import data\r\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/andrybrew/bi-8-maret/main/data/tweet-full.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9lNfxTM1wV0"
      },
      "source": [
        "# Lihat 5 baris pertama data\r\n",
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfntSX4DopwQ"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIXInlqooGaO"
      },
      "source": [
        "### **Text Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PtaspkU8ei5"
      },
      "source": [
        "# Pilih 5 kolom teks saja\r\n",
        "tweet = df[['text']]\r\n",
        "\r\n",
        "# Lihat 5 baris pertama data\r\n",
        "tweet.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmpAtGBFFGCG"
      },
      "source": [
        "### **Transformation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewjn6JMbDejZ"
      },
      "source": [
        "# Membuat fungsi transformasi tweet\r\n",
        "def transform_tweet(row):\r\n",
        "  tweet = row['text']\r\n",
        "  tweet = p.clean(tweet)\r\n",
        "  tweet = str.lower(tweet)\r\n",
        "  return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMa_w71GEaFS"
      },
      "source": [
        "# Mengaplikasikan fungsi transofrmasi\r\n",
        "tweet['transformed'] = tweet.apply(transform_tweet, axis=1)\r\n",
        "\r\n",
        "# Lihat 5 baris pertama data\r\n",
        "tweet.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0ZiliHSFSnH"
      },
      "source": [
        "### **Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0l9wvMPFpNE"
      },
      "source": [
        "# Download Punkt\r\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJfyNe2VC6rN"
      },
      "source": [
        "# Membuat fungsi tokenization\r\n",
        "def tokenize_tweet(row):\r\n",
        "    tweet = row['transformed']\r\n",
        "    tokens = nltk.word_tokenize(tweet)\r\n",
        "    token_words = [w for w in tokens if w.isalpha()]\r\n",
        "    return token_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFmjyoGi_rxN"
      },
      "source": [
        "# Mengaplikasikan fungsi tokenization\r\n",
        "tweet['tokenized'] = tweet.apply(tokenize_tweet, axis=1)\r\n",
        "\r\n",
        "# Lihat 5 baris pertama data\r\n",
        "tweet.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtKXiR7YF9RZ"
      },
      "source": [
        "### **Lemmatization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNKhzkZvF7mR"
      },
      "source": [
        "# Download Wordnet\r\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXSeJu9gGK5e"
      },
      "source": [
        "# Membuat fungsi lemmatization\r\n",
        "def lemmatize_tweet(row):\r\n",
        "    list = row['tokenized']\r\n",
        "    lemmatize_list = [WordNetLemmatizer().lemmatize(w, pos='v') for w in list]\r\n",
        "    return(lemmatize_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZm2bfvaGjyu"
      },
      "source": [
        "# Mengaplikasikan fungsi lemmatization\r\n",
        "tweet['lemmatized'] = tweet.apply(lemmatize_tweet, axis=1)\r\n",
        "\r\n",
        "# Lihat 5 baris pertama data\r\n",
        "tweet.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zeuh1xNYIzy5"
      },
      "source": [
        "### **Stopword Removal**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7XcHI7VIwjW"
      },
      "source": [
        "# Download stopword bahasa inggris\r\n",
        "nltk.download('stopwords')\r\n",
        "stops = set(stopwords.words(\"english\"))     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvUidpdOBuBU"
      },
      "source": [
        "# Membuat fungsi lemmatization\r\n",
        "def stopword_tweet(row):\r\n",
        "    list = row['lemmatized']\r\n",
        "    stopword_list = [w for w in list if not w in stops]\r\n",
        "    return(stopword_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPei0h4uCEh5"
      },
      "source": [
        "# Mengaplikasikan fungsi Stopword\r\n",
        "tweet['stopword'] = tweet.apply(stopword_tweet, axis=1)\r\n",
        "\r\n",
        "# Lihat 5 baris pertama data\r\n",
        "tweet.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAkl9x-LI4HL"
      },
      "source": [
        "### **Rejoin**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGzqcONlCHYo"
      },
      "source": [
        "# Membuat fungsi rejoin\r\n",
        "def rejoin_tweet(row):\r\n",
        "    list = row['stopword']\r\n",
        "    joined_words = ( \" \".join(list))\r\n",
        "    return joined_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GkNjo6bCc36"
      },
      "source": [
        "# Mengaplikasikan fungsi rejoin\r\n",
        "tweet['final'] = tweet.apply(rejoin_tweet, axis=1)\r\n",
        "\r\n",
        "# Lihat 5 baris pertama data\r\n",
        "tweet.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OkxLRK7JwRZ"
      },
      "source": [
        "# Final tweet yang sudah di proses\r\n",
        "tweet_clean = tweet[['final']]\r\n",
        "tweet_clean = tweet_clean.rename(columns={'final': 'text'})\r\n",
        "\r\n",
        "# Lihat 5 baris pertama data\r\n",
        "tweet_clean.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_YvObORoOm8"
      },
      "source": [
        "### **Wordcloud**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmiFeAwzoOK2"
      },
      "source": [
        "# Visualisasi Word Cloud\r\n",
        "text_wordcloud = \" \".join(tweet for tweet in tweet_clean.text)\r\n",
        "\r\n",
        "cloud = WordCloud(background_color='white').generate(text_wordcloud)\r\n",
        "\r\n",
        "plt.figure(figsize=(10, 10), facecolor=None)\r\n",
        "plt.imshow(cloud, interpolation=\"bilinear\")\r\n",
        "plt.axis(\"off\")\r\n",
        "plt.tight_layout(pad=0)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VylHW2_Um_eo"
      },
      "source": [
        "### **Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqUKHLCim63Q"
      },
      "source": [
        "# Download corpus untuk sentiment analysis\r\n",
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWHhRbz0nJ6J"
      },
      "source": [
        "# Sentiment Analysis\r\n",
        "sid = SentimentIntensityAnalyzer()\r\n",
        "listy = [] \r\n",
        "for index, row in tweet_clean.iterrows():\r\n",
        "  ss = sid.polarity_scores(row['text'])\r\n",
        "  listy.append(ss)\r\n",
        "  \r\n",
        "se = pd.Series(listy)\r\n",
        "tweet_clean['polarity'] = se.values\r\n",
        "display(tweet_clean.head(5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kE7V6DNsMf9p"
      },
      "source": [
        "# Visualisasi Pie Chart\r\n",
        "labels = ['negative', 'neutral', 'positive']\r\n",
        "sizes  = [ss['neg'], ss['neu'], ss['pos']]\r\n",
        "plt.pie(sizes, labels=labels, autopct='%1.1f%%')\r\n",
        "plt.axis('equal') \r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvLdU2GGoY_d"
      },
      "source": [
        "### **Topic Modelling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSyZiXaiNI3P"
      },
      "source": [
        "# clone tambahan library dari github\r\n",
        "! git clone https://github.com/machine-learning-ss/tm\r\n",
        "\r\n",
        "# Set Data Directory\r\n",
        "os.chdir('tm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7sbfDlWNJUp"
      },
      "source": [
        "import MyLib as TS\r\n",
        "\r\n",
        "Tweets = tweet_clean['text']\r\n",
        "print('Total loaded tweets = {0}'.format(len(Tweets)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPBZxuW0NYoH"
      },
      "source": [
        "n_topics = 4\r\n",
        "top_topics = 4\r\n",
        "top_words = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_mznEgLNvK2"
      },
      "source": [
        "# Feature Extraction\r\n",
        "count_vector = CountVectorizer(token_pattern = r'\\b[a-zA-Z]{3,}\\b') \r\n",
        "dtm_tf = count_vector.fit_transform(Tweets)\r\n",
        "tf_terms = count_vector.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1NgY3twNwBB"
      },
      "source": [
        "# Fungsi untuk mencari topic\r\n",
        "lda_tf = LatentDirichletAllocation(n_components=n_topics, learning_method='online', random_state=0).fit(dtm_tf)\r\n",
        "\r\n",
        "# Menampilkan Topik\r\n",
        "vsm_topics = lda_tf.transform(dtm_tf); doc_topic =  [a.argmax()+1 for a in tqdm(vsm_topics)] # topic of docs\r\n",
        "print('In total there are {0} major topics, distributed as follows'.format(len(set(doc_topic))))\r\n",
        "plt.hist(np.array(doc_topic), alpha=0.5); plt.show()\r\n",
        "print('Printing top {0} Topics, with top {1} Words:'.format(top_topics, top_words))\r\n",
        "TS.print_Topics(lda_tf, tf_terms, top_topics, top_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOcvEGgENy0f"
      },
      "source": [
        "# Visualisasi Topic Secara Interaktif\r\n",
        "pyLDAvis.sklearn.prepare(lda_tf, dtm_tf, count_vector) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B07sph-yRgrI"
      },
      "source": [
        "### **Text Network Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9Ue_Hm2R-vV"
      },
      "source": [
        "# Pilih teks\r\n",
        "text = tweet_clean['text']\r\n",
        "text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj-AW6a3SP3c"
      },
      "source": [
        "# Tokenize\r\n",
        "text_data = [word_tokenize(i) for i in text]\r\n",
        "print(text_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nqqlH4NTgyK"
      },
      "source": [
        "# Membuat fungsi cooccurence\r\n",
        "def generate_co_occurrence_matrix(corpus):\r\n",
        "    vocab = set(corpus)\r\n",
        "    vocab = list(vocab)\r\n",
        "    vocab_index = {word: i for i, word in enumerate(vocab)}\r\n",
        "\r\n",
        "    bi_grams = list(bigrams(corpus))\r\n",
        "    bigram_freq = nltk.FreqDist(bi_grams).most_common(len(bi_grams))\r\n",
        " \r\n",
        "    co_occurrence_matrix = np.zeros((len(vocab), len(vocab)))\r\n",
        " \r\n",
        "    for bigram in bigram_freq:\r\n",
        "        current = bigram[0][1]\r\n",
        "        previous = bigram[0][0]\r\n",
        "        count = bigram[1]\r\n",
        "        pos_current = vocab_index[current]\r\n",
        "        pos_previous = vocab_index[previous]\r\n",
        "        co_occurrence_matrix[pos_current][pos_previous] = count\r\n",
        "    co_occurrence_matrix = np.matrix(co_occurrence_matrix)\r\n",
        " \r\n",
        "    return co_occurrence_matrix, vocab_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJnRvQEuTwPz"
      },
      "source": [
        "# Membuat Adjacency Matrix\r\n",
        "data = list(itertools.chain.from_iterable(text_data))\r\n",
        "matrix, vocab_index = generate_co_occurrence_matrix(data)\r\n",
        " \r\n",
        " \r\n",
        "data_matrix = pd.DataFrame(matrix, index=vocab_index,\r\n",
        "                             columns=vocab_index)\r\n",
        "\r\n",
        "# Show Adjacency Matrix\r\n",
        "data_matrix.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9zsB-9qT0sA"
      },
      "source": [
        "# Membuat Network dengan Adjacency Matrix\r\n",
        "G = nx.from_pandas_adjacency(data_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbJuj86QZrwr"
      },
      "source": [
        "# Degree Centrality\r\n",
        "degree = nx.degree_centrality(G)\r\n",
        "\r\n",
        "list_node = list(degree) \r\n",
        "selected_node = list_node[1:600]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYCBGhCoT5ym"
      },
      "source": [
        "sampled_graph = G.subgraph(selected_node)\r\n",
        "\r\n",
        "plt.figure(figsize=(10, 10), facecolor=None)\r\n",
        "nx.draw(sampled_graph, with_labels=True, \r\n",
        "        node_color='skyblue', node_size=600, \r\n",
        "        arrowstyle='->',arrowsize=20, edge_color='r',\r\n",
        "        font_size=7,\r\n",
        "        pos=nx.kamada_kawai_layout(sampled_graph))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
